<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Ling Wang</title>

  <meta name="author" content="Ling Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

</head>


<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p style="text-align:left">
                    <name>Ling Wang | 汪羚</name>
                  </p>
                  <br>
                  <p style="line-height: 1.6">
                    I am a fourth-year PhD student at Xi'an Research Institute of Hi-Tech</a> and <a
                      href="https://www.cs.tsinghua.edu.cn/">Department of Computer Science, Tsinghua University</a>,
                    advised by <a href="https://www.cs.tsinghua.edu.cn/csen/info/1154/3972.htm">Prof.Fuchun Sun</a>.
                    I obtained my B.S. and M.S at Xi'an Research Institute of Hi-Tech.
                  </p>
                  <p style="line-height: 1.6">
                    My research interest lies in the <b>3D Reconstruction</b>, <b>3D Generation</b>, and
                    <b>Robotics</b>. I am particularly interested in combining Vision Language Action model with
                    3D generation methods.
                  </p>
                  <p style="text-align:left">
                    <a href="mailto:yanyuwangl@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/2019EPWL"> Github </a>
                  </p>
                </td>
                <td style="padding:3%;width:40%;max-width:40%">
                  <img style="width:70%;max-width:70%" alt="profile photo" src="images/IMG_9176.JPG"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>News</heading>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
         <li style="margin: 20px;">
            <b>2026-01:</b> SceneTransporter is accepted by ICLR 2026</a>.
          </li>
         <li style="margin: 20px;">
            <b>2025-01:</b> EquiShape is accepted by IEEE TIP</a>.
          </li>
          <li style="margin: 20px;">
            <b>2024-07:</b> Work on 3D AIGC is accepted by ECCV 2024</a>.
          </li>
          <li style="margin: 20px;">
            <b>2024-05:</b> SMERL is accepted by ICML 2024.
          </li>
          <li style="margin: 20px;">
            <b>2023-07:</b> Work on 3D Keypoint Discovery is accepted by ICCV 2023.
          </li>
          <li style="margin: 20px;">
            <b>2023-01:</b> Work on Depth Estimation is accepted by ICRA 2023.
          </li>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>Publications</heading>
                  </p>
                  <p>
                    * indicates equal contribution
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/hunyuan3d-2.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation
                  </papertitle>
                  <br>
                  <strong>Ling Wang</strong>, Hao-Xiang Guo, Xinzhou Wang, Fuchun Sun, Kai Sun, Pengkun Liu, Hang Xiao, Zhong Wang, Guangyuan Fu, Eric Li, Yang Liu, Yikai Wang
                  <br>
                  <em>ICLR, 2026</em>
                  <br>
                  <a
                    href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf">[PDF]</a>
                  <a href="https://github.com/Tencent/Hunyuan3D-2">[Code]</a>
                  <a href="https://3d-models.hunyuan.tencent.com/">[Project Page]</a>
                  <br>
                  <p> We propose Tencent Hunyuan3D-2.0, a unified framework for text-to-3D and image-to-3D generation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Equivariant.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape
                    Correspondence</papertitle>
                  <br>
                  <strong>Ling Wang</strong>, Runfa Chen, Yikai Wang, Fuchun Sun, Xinzhou Wang, Sun Kai, Guangyuan Fu,
                  Jianwei Zhang, Wenbing Huang

                  <br>
                  <em>IEEE Transactions on Image Processing (TIP), 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.00959">[arXiv]</a>

                  <br>
                  <p> We present a framework for unsupervised non-rigid point cloud shape correspondence registration,
                    incorporating principles of equivariance.</p>

                </td>
              </tr>
              

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/pipeline2.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation
                    and Reconstruction with Canonical Score Distillation</papertitle>
                  <br>
                  Xinzhou Wang,
                  Yikai Wang,
                  Junliang Ye,
                  Zhengyi Wang,
                  Fuchun Sun,
                  Pengkun Liu,
                  <strong>Ling Wang</strong>,
                  Kai Sun,
                  Xintong Wang,
                  Bin He
                  <br>
                  <em>ECCV, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2312.03795">[arXiv]</a>
                  <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">[Code]</a>
                  <a href="https://zz7379.github.io/AnimatableDreamer">[Project Page]</a>
                  <br>
                  <p> We propose AnimatableDreamer, a framework with the capability to generate generic categories of
                    non-rigid 3D models.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/pipeline.jpg" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments</papertitle>
                  <br>
                  Runfa Chen, <strong>Ling Wang</strong>, Yu Du, Tianrui Xue, Fuchun Sun, Jianwei Zhang
                  <br>
                  <em>ICML, 2024</em>
                  <br>
                  <a href="http://arxiv.org/abs/2403.14613">[arXiv]</a>
                  <a href="https://github.com/liuff19/DreamReward">[Code]</a>
                  <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a>
                  <br>
                  <p> We present a comprehensive framework, coined DreamReward, to
                    learn and improve text-to-3D models from human preference feedback.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Vidu4D.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>3D Implicit Transporter for Temporally Consistent Keypoint Discovery</papertitle>
                  <br>
                  Yikai Wang*, <strong>Xinzhou Wang*</strong>, Zilong Chen, Zhengyi Wang, Fuchun Sun, Jun Zhu

                  <br>
                  <em>NeurIPS, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2405.16822">[arXiv]</a>
                  <a href="https://github.com/yikaiw/vidu4d">[Code]</a>
                  <a href="https://vidu4d-dgs.github.io/">[Project Page]</a>
                  <a href="https://www.youtube.com/watch?v=4tUkDj3pglg/">[Youtube]</a>
                  <br>
                  <p> We present a Vidu4D, a framework to reconstruct high-fidelity 4D model from generated videos.</p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Digital.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Digital-Twin-Assisted Skill Learning for 3C Assembly Tasks</papertitle>
                  <br>
                  Fuchun Sun</strong>, Naijun Liu, <strong>Xinzhou Wang</strong>, Ruize Sun, Shengyi Miao, Zengxin Kang,
                  Bin Fang, Huaping Liu, Yongjia Zhao, Haiming Huang

                  <br>
                  <em>IEEE Transactions on Cybernetics.</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10492985/">[paper]</a>

                  <br>
                  <p> We present a digital twin with reinforce learning for sim-to-real 3C assembly task.</p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/3C.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Phys-Field: Physics-Enhanced Neural Surfaces for Sim(3) Estimation in Robotic Assembly
                    Using a Single RGB Sensor</papertitle>
                  <br>
                  <strong>Xinzhou Wang</strong>, Fuchun Sun, Ling Wang, Kai Sun, Yuanyan Xie, Xintong Wang, Bin He,
                  Huaidong Zhou

                  <br>
                  <em>IEEE Sensors Journal</em>
                  <br>
                  <a href="https://arxiv.org">[arXiv]</a>

                  <br>
                  <p> We present a implict digital twin with physics engine for sim-to-real 3C assembly task.</p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/pipeline_iso.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</papertitle>
                  <br>
                  Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, <strong>Xinzhou
                    Wang</strong>
                  <br>
                  <em>Arxiv, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2403.10395">[arXiv]</a>
                  <a href="https://isotropic3d.github.io/">[Code]</a>
                  <a href="https://github.com/pkunliu/Isotropic3D">[Project Page]</a>
                  <br>
                  <p> We propose Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding
                    as input.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/tongue.jpg" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Deep Learning Based Tongue Prickles Detection in Traditional Chinese Medicine</papertitle>
                  <br>
                  <strong>Xinzhou Wang</strong>, Siyan Luo, Guihua Tian, Xiangrong Rao, Bin He, Fuchun Sun
                  <br>
                  <em>Evidence-Based Complementary and Alternative Medicine, 2022</em>
                  <br>
                  <a href="https://www.hindawi.com/journals/ecam/2022/5899975/">[Paper]</a>
                  <br>
                  <p> We provides a quantitative perspective for symptoms and disease diagnosis according to tongue
                    characteristics.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:80%;max-width:80%" src="images/engine.gif" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Measurement Selection Method for Aero-engine Discrete Operating Conditions Gas Path
                    Analysis</papertitle>
                  <br>
                  Xintong Wang, <strong>Xinzhou Wang</strong>
                  <br>
                  <em>International Conference on Algorithms, Data Mining, and Information Technology (ADMIT), 2022</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9974728">[Paper]</a>
                  <br>
                  <p> We provides a method for measurement selection of Aero-engine gas path analysis.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:80%;max-width:80%" src="images/cos.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>Camouflaged Object Segmentation with Transformer</papertitle>
                  <br>
                  Haiwen Wang*, <strong>Xinzhou Wang*</strong>, Fuchun Sun, Yixu Song
                  <br>
                  <em> Cognitive Systems and Information Processing, 2021</em>
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-981-16-9247-5_17">[Paper]</a>
                  <br>
                  <p> We provides a transformer for camouflaged object segmentation.</p>
                </td>
              </tr>

              <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>CVPR 2023</b>.
              </li>
          </td>
        </tr> -->
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>

  <p>
    <center>
      <div id="clustrmaps-widget" style="width:5%">
        <script type="text/javascript" id="clstr_globe"
          src="//clustrmaps.com/globe.js?d=0E8MTgY3uzPlMqQ6k4ja-Y8ajcyDF48Pd6VYZ5EfX_A"></script>
        <br>
        &copy; Xinzhou Wang | Last updated: 25 Mar, 2024
    </center>
  </p>
</body>

</html>
