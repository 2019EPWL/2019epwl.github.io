<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Ling Wang</title>

  <meta name="author" content="Ling Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

</head>


<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p style="text-align:left">
                    <name>Ling Wang | 汪羚</name>
                  </p>
                  <br>
                  <p style="line-height: 1.6">
                    I am a fourth-year PhD student at Xi'an Research Institute of Hi-Tech</a> and <a
                      href="https://www.cs.tsinghua.edu.cn/">Department of Computer Science, Tsinghua University</a>,
                    advised by <a href="https://www.cs.tsinghua.edu.cn/csen/info/1154/3972.htm">Prof.Fuchun Sun</a>.
                    I obtained my B.S. and M.S at Xi'an Research Institute of Hi-Tech.
                  </p>
                  <p style="line-height: 1.6">
                    My research interest lies in the <b>3D Reconstruction</b>, <b>3D Generation</b>, and
                    <b>Robotics</b>. I am particularly interested in combining Vision Language Action model with
                    3D generation methods.
                  </p>
                  <p style="text-align:left">
                    <a href="mailto:yanyuwangl@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/2019EPWL"> Github </a>
                  </p>
                </td>
                <td style="padding:3%;width:40%;max-width:40%">
                  <img style="width:70%;max-width:70%" alt="profile photo" src="images/IMG_9176.JPG"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>News</heading>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
         <li style="margin: 20px;">
            <b>2026-01:</b> SceneTransporter is accepted by ICLR 2026</a>.
          </li>
         <li style="margin: 20px;">
            <b>2025-01:</b> EquiShape is accepted by IEEE TIP</a>.
          </li>
          <li style="margin: 20px;">
            <b>2024-07:</b> Work on 3D AIGC is accepted by ECCV 2024</a>.
          </li>
          <li style="margin: 20px;">
            <b>2024-05:</b> SMERL is accepted by ICML 2024.
          </li>
          <li style="margin: 20px;">
            <b>2023-07:</b> Work on 3D Keypoint Discovery is accepted by ICCV 2023.
          </li>
          <li style="margin: 20px;">
            <b>2023-01:</b> Work on Depth Estimation is accepted by ICRA 2023.
          </li>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>Publications</heading>
                  </p>
                  <p>
                    * indicates equal contribution
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/hunyuan3d-2.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation
                  </papertitle>
                  <br>
                  <strong>Ling Wang*</strong>, Hao-Xiang Guo*, Xinzhou Wang*, Fuchun Sun, Kai Sun, Pengkun Liu, Hang Xiao, Zhong Wang, Guangyuan Fu, Eric Li, Yang Liu, Yikai Wang
                  <br>
                  <em>ICLR, 2026</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.00959">[arXiv]</a>
                  <a href="https://3d-models.hunyuan.tencent.com/">[Project Page]</a>
                  <br>
                  <p> We propose Tencent Hunyuan3D-2.0, a unified framework for text-to-3D and image-to-3D generation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Equivariant.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape
                    Correspondence</papertitle>
                  <br>
                  <strong>Ling Wang*</strong>, Runfa Chen*, Yikai Wang, Fuchun Sun, Xinzhou Wang, Sun Kai, Guangyuan Fu,
                  Jianwei Zhang, Wenbing Huang

                  <br>
                  <em>IEEE Transactions on Image Processing (TIP), 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.00959">[arXiv]</a>

                  <br>
                  <p> We present a framework for unsupervised non-rigid point cloud shape correspondence registration,
                    incorporating principles of equivariance.</p>

                </td>
              </tr>
              

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/pipeline2.png" alt="dise">
                </td>
                <td width="75%" valign="center">
                  <papertitle>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation
                    and Reconstruction with Canonical Score Distillation</papertitle>
                  <br>
                  Xinzhou Wang,
                  Yikai Wang,
                  Junliang Ye,
                  Zhengyi Wang,
                  Fuchun Sun,
                  Pengkun Liu,
                  <strong>Ling Wang</strong>,
                  Kai Sun,
                  Xintong Wang,
                  Bin He
                  <br>
                  <em>ECCV, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2312.03795">[arXiv]</a>
                  <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">[Code]</a>
                  <a href="https://zz7379.github.io/AnimatableDreamer">[Project Page]</a>
                  <br>
                  <p> We propose AnimatableDreamer, a framework with the capability to generate generic categories of
                    non-rigid 3D models.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/pipeline.jpg" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments</papertitle>
                  <br>
                  Runfa Chen*, <strong>Ling Wang*</strong>, Yu Du, Tianrui Xue, Fuchun Sun, Jianwei Zhang
                  <br>
                  <em>ICML, 2024</em>
                  <br>
                  <a href="http://arxiv.org/abs/2403.14613">[arXiv]</a>
                  <a href="https://github.com/liuff19/DreamReward">[Code]</a>
                  <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a>
                  <br>
                  <p> We present a comprehensive framework, coined DreamReward, to
                    learn and improve text-to-3D models from human preference feedback.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Vidu4D.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>3D Implicit Transporter for Temporally Consistent Keypoint Discovery</papertitle>
                  <br>
                  Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, Li Yi, Xiaodong Mu, <strong>Ling Wang<strong>, Pengfei Li, Guyue Zhou, Chao Yang, Xinliang Zhang, Jian Zhao

                  <br>
                  <em>ICCV, 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2405.16822">[arXiv]</a>
                  <a href="https://github.com/yikaiw/vidu4d">[Code]</a>
                  <a href="https://vidu4d-dgs.github.io/">[Project Page]</a>
                  <a href="https://www.youtube.com/watch?v=4tUkDj3pglg/">[Youtube]</a>
                  <br>
                  <p> We present a Vidu4D, a framework to reconstruct high-fidelity 4D model from generated videos.</p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;max-width:30%" align="center">
                  <img style="width:100%;max-width:100%" src="images/Digital.png" alt="dise">
                </td>
                <td width="80%" valign="center">
                  <papertitle>STEPS: Joint Self-supervised Nighttime Image Enhancement and Depth Estimation</papertitle>
                  <br>
                  Yupeng Zheng, Chengliang Zhong, Pengfei Li, Huan-ang Gao, Yuhang Zheng, Bu Jin, <strong>Ling Wang<strong>, Hao Zhao, Guyue Zhou, Qichao Zhang and Dongbin Zhao

                  <br>
                  <em>ICRA, 2023.</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10492985/">[paper]</a>

                  <br>
                  <p> We present a digital twin with reinforce learning for sim-to-real 3C assembly task.</p>

                </td>
              </tr>

              

              <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>CVPR 2023</b>.
              </li>
          </td>
        </tr> -->
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>

  <p>
    <center>
      <div id="clustrmaps-widget" style="width:5%">
        <script type="text/javascript" id="clstr_globe"
          src="//clustrmaps.com/globe.js?d=0E8MTgY3uzPlMqQ6k4ja-Y8ajcyDF48Pd6VYZ5EfX_A"></script>
        <br>
        &copy; Ling Wang | Last updated: 25 Feb, 2026
    </center>
  </p>
</body>

</html>
